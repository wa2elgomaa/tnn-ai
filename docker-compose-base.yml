services:
  # es01:
  #   container_name: tnnai-es-01
  #   profiles:
  #     - elasticsearch
  #   image: elasticsearch:${STACK_VERSION}
  #   volumes:
  #     - esdata01:/usr/share/elasticsearch/data
  #   ports:
  #     - ${ES_PORT}:9200
  #   env_file: .env
  #   environment:
  #     - node.name=es01
  #     - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
  #     - bootstrap.memory_lock=false
  #     - discovery.type=single-node
  #     - xpack.security.enabled=true
  #     - xpack.security.http.ssl.enabled=false
  #     - xpack.security.transport.ssl.enabled=false
  #     - cluster.routing.allocation.disk.watermark.low=5gb
  #     - cluster.routing.allocation.disk.watermark.high=3gb
  #     - cluster.routing.allocation.disk.watermark.flood_stage=2gb
  #     - TZ=${TIMEZONE}
  #   mem_limit: ${MEM_LIMIT}
  #   ulimits:
  #     memlock:
  #       soft: -1
  #       hard: -1
  #   healthcheck:
  #     test: ["CMD-SHELL", "curl http://localhost:9200"]
  #     interval: 10s
  #     timeout: 10s
  #     retries: 120
  #   networks:
  #     -  tnn_ai
  #   restart: on-failure

  # opensearch01:
  #   container_name: tnnai-opensearch-01
  #   profiles:
  #     - opensearch
  #   image: hub.icert.top/opensearchproject/opensearch:2.19.1
  #   volumes:
  #     - osdata01:/usr/share/opensearch/data
  #   ports:
  #     - ${OS_PORT}:9201
  #   env_file: .env
  #   environment:
  #     - node.name=opensearch01
  #     - OPENSEARCH_PASSWORD=${OPENSEARCH_PASSWORD}
  #     - OPENSEARCH_INITIAL_ADMIN_PASSWORD=${OPENSEARCH_PASSWORD}
  #     - bootstrap.memory_lock=false
  #     - discovery.type=single-node
  #     - plugins.security.disabled=false
  #     - plugins.security.ssl.http.enabled=false
  #     - plugins.security.ssl.transport.enabled=true
  #     - cluster.routing.allocation.disk.watermark.low=5gb
  #     - cluster.routing.allocation.disk.watermark.high=3gb
  #     - cluster.routing.allocation.disk.watermark.flood_stage=2gb
  #     - TZ=${TIMEZONE}
  #     - http.port=9201
  #   mem_limit: ${MEM_LIMIT}
  #   ulimits:
  #     memlock:
  #       soft: -1
  #       hard: -1
  #   healthcheck:
  #     test: ["CMD-SHELL", "curl http://localhost:9201"]
  #     interval: 10s
  #     timeout: 10s
  #     retries: 120
  #   networks:
  #     -  tnn_ai
  #   restart: on-failure

  # postgres:
  #   image: pgvector/pgvector:pg16
  #   container_name: tnnai-pg
  #   # restart: unless-stopped
  #   environment:
  #     POSTGRES_DB: ${POSTGRES_DBNAME}
  #     POSTGRES_USER: ${POSTGRES_USER}
  #     POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
  #   ports:
  #     - "${POSTGRES_PORT}:5432"
  #   networks:
  #     - tnn_ai
  #   volumes:
  #     - postgres_data:/var/lib/postgresql/data/ #using a volume

  redis:
    # swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/valkey/valkey:8
    image: valkey/valkey:8
    container_name: tnnai-redis
    command: redis-server --requirepass ${REDIS_PASSWORD} --maxmemory 128mb --maxmemory-policy allkeys-lru
    env_file: .env
    ports:
      - ${REDIS_PORT}:6379
    volumes:
      - redis_data:/data
    networks:
      - tnn_ai
    restart: on-failure
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD}", "ping"]
      interval: 5s
      timeout: 3s
      retries: 3
      start_period: 10s

  vllm:
    image: vllm/vllm-openai:latest
    container_name: vllm-server
    restart: always
    ports:
      - "8081:8000" # host:container (vLLM defaults to 8000)
    environment:
      # Load environment variables from .env file automatically
      VLLM_USE_V1: "1" # ensure correct async engine
      TRANSFORMERS_OFFLINE: "0"
      HUGGING_FACE_HUB_TOKEN: ${HF_HUB_TOKEN}
      VLLM_LOGGING_LEVEL: ${VLLM_LOGGING_LEVEL}
      VLLM_USE_CPU: "1"
    volumes:
      - ./models:/models # local model directory
      - ~/.cache/huggingface:/root/.cache/huggingface # Hugging Face cache
    command: >
      --model ${VLLM_MODEL_ID}
      --port ${VLLM_PORT}
      --host ${VLLM_HOST}
      --gpu-memory-utilization ${VLLM_GPU_MEMORY_UTILIZATION}
      --max-num-batched-tokens ${VLLM_MAX_NUM_BATCHED_TOKENS}


  ollama: # optional local LLM support
    platform: linux/amd64
    image: ollama/ollama
    environment:
      - OLLAMA_NUM_GPU=${OLLAMA_NUM_GPU}
      # - OLLAMA_KEEP_ALIVE=${OLLAMA_KEEP_ALIVE}
    deploy:
      resources:
        limits:
          memory: 16g
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama

volumes:
  ollama_data:
    driver: local
  esdata01:
    driver: local
  osdata01:
    driver: local
  infinity_data:
    driver: local
  mysql_data:
    driver: local
  minio_data:
    driver: local
  redis_data:
    driver: local
  postgres_data:
    driver: local

networks:
  tnn_ai:
    driver: bridge
